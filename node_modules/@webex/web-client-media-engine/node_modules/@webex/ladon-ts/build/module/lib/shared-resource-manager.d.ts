import * as ort from 'onnxruntime-web';
import { PipelineConfig, PreloadConfig, SizeConfig } from './config';
/**
 * Shared resource manager used to preload and share resources across different pipelines.
 * This is useful when multiple pipelines are using the same model or when the model needs
 * to be preloaded before the pipeline is created.
 */
declare class SharedResourceManager {
    private modelSessions;
    private worker?;
    private workerUrl?;
    /**
     * Initialize the worker using the provided configuration.
     *
     * @param {PreloadConfig} config The configuration object.
     * @returns {Promise<Worker>} A promise that resolves when the worker is initialized.
     * @example
     * await initializeWorker(config);
     */
    initializeWorker(config: PreloadConfig): Promise<Worker>;
    /**
     * Terminate the worker.
     */
    terminateWorker(): void;
    /**
     * Preload the model using the provided configuration.
     *
     * @param {PreloadConfig} config The configuration object.
     * @returns {Promise<void>} A promise that resolves when the model is preloaded.
     * @example
     * await preloadModel(config);
     */
    preloadModel(config: PreloadConfig): Promise<ort.InferenceSession>;
    /**
     * Preload the worker using the provided configuration.
     *
     * @param {PreloadConfig} config The configuration object.
     * @returns {Promise<void>} A promise that resolves when the worker is preloaded.
     * @example
     * await preloadWorker(config);
     */
    preloadWorker(config: PreloadConfig): Promise<void>;
    /**
     * Warmup the model session with a dummy input to avoid latency when running the model for the first time.
     *
     * @param {ort.InferenceSession} session The model session.
     * @param {SizeConfig} inputSize The input size of the model.
     * @param {number} inputChannels The number of input channels (e.g., RGB is 3).
     * @param {ort.Tensor.Type} inputType The input tensor type.
     * @returns {Promise<void>} A promise that resolves when the model is warmed up.
     * @example
     * await warmupModel(session, inputSize, inputChannels, inputType);
     */
    warmupModel(session: ort.InferenceSession, inputSize?: SizeConfig, inputChannels?: number, inputType?: ort.Tensor.Type): Promise<void>;
    /**
     * Get the model session for the provided model URL.
     *
     * @param {string} modelUrl The model URL.
     * @returns {ort.InferenceSession} The model session.
     * @example
     * const session = getModelSession(modelUrl);
     */
    getModelSession(modelUrl: string): ort.InferenceSession;
    /**
     * Resolves the model URL based on the configuration.
     *
     * @param {PipelineConfig} config The configuration object.
     * @returns {Promise<string>} The resolved model URL.
     * @example
     * const modelUrl = await getModelUrl(config);
     */
    getModelUrl({ mask: { model_url_resolver, model_uri } }: PipelineConfig | PreloadConfig): Promise<string>;
    getWorker(): Worker | undefined;
    getWorkerUrl(): string | undefined;
}
export declare const sharedResourceManager: SharedResourceManager;
/**
 * Preload the resources using the provided configuration. This function is used to preload the model
 * and the worker before creating the pipeline.
 *
 * @param config - The configuration object.
 */
export declare const preload: (config: PreloadConfig) => Promise<void>;
export {};
